-Onderzoek doen naar manier waarop ondertiteling gemaakt word
-Iets met timing doen, blijkbaar staat de ondertiteling langer in beeld als er ruimte voor is om de kijker meer tijd te geven om te lezen
-https://over.npo.nl/storage/configurations/overnpo/files/bijlage_richtlijnen_ondertiteling.pdf

Klein probleempje:
-Ik heb heel veel getraind met data waarin ik ook de reclames toelaat. 
Ik heb ook geprobeerd de dubbele data eruit te halen, wat opzich goed gebeurt. Het ding is dat binnen de testdata nogsteeds heel veel reclame zit,
reclame word natuurlijk altijd in andere volgorde afgespeeld, waardoor er nogsteeds dubbele data doorheen komt in de vorm van reclames.
Dubbele uitzendingen hebben we niet echt

-Ik heb het kunnen fixen door de als testdata alleen 'kwaliteit' data te gebruiken, die schaars is, maar wel reclameloos.
De resultaten zijn dat we minder goede captions verzamelen, maar nogsteeds heb ik het gevoel dat het data is die al eerder is gezien
door de trainer...

Correcte , gevallen delen door correcte , gevallen:
-comma.txt

eh gevallen zoeken in analyse:
-Ik heb een scriptje geschreven die een bepaald pattern vind in 'bestand' met gebruik van grep, waardoor ik alle lines kan verzamelen
die dit patroon bevatten. Vervolgens is dit handig voor een evaluatie zodat we kunnen zien hoe het model werkt als hij dit specifieke patroon ziet
-Ik heb dit gedaan voor 'uh' voor 'bestand'=asr-train.txt, omdat ik erachter kwam dat uh vaak werd gebruikt in de asr. Voor het model dat 8 uur
trainingsdata (met alle filters) had gaf dit een WER van 0.15, de resultaten van de modellen van 16 en 32 uur gaven een veel hogere WER van rond 
de 0.4. Voor het model van 8 uur met alle filters is het opzich logisch dat de resultaten beter zijn, omdat dit ondertiteling is van hoge kwaliteit
niet van reclames en live uitzendingen etc, waarin het woord uh waarschijnlijk vaker word goedgekeurd
-Oke toch misschien niet helemaal, want ik mag natuurlijk geen data testen uit de train dataset. Dus ik heb deze test opnieuw gedaan maar dan op
de test dataset van c2tempdata3 omdat daar veel 'uh' samples tussen zaten. De performance hiervan is alsnog redelijk goed. Al heb ik niet meer
de drastische verandering tussen 8 en 16 + 32 uur. Wat opzich veel te goed voor woorden leek, dus wss alsnog noise
-c2tempdata3 - test - uh-analysis

Spelen met lm weight en word penalty:
-base = lmweight = 2 and wordpenalty = -1
-lmweight [0,5], word [-2,2]
-c2tempdata3 - test - var-analysis

Extra experimenten
-xlsr
-tijden van text afhalen, laten lopen met beste WER in vergelijking met de asr, gewoon met c2tempdata, 2 en 3
